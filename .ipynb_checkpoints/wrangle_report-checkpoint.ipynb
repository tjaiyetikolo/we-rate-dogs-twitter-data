{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c5244d",
   "metadata": {},
   "source": [
    "## Data Wrangling Process\n",
    "\n",
    "This project made me utlise a lot of skills and efforts towards the data phase which began from the introduction of the three datasets: `twitter_archive`, `image_predict`, and `tweet_df` which were gathered from a range of sources. The `twitter_archive` dataset was provided as a csv from the start of this project, while the predictions file `image_predict` had to downloaded programmatically from a provided url. This made use of the requests module in obtaining the content and the os module in saving it as a .tsv file to the loacal device used. The last dataset was to be derived using the Twitter API which had to have access to a developer account to pull the tweets required for the tweet_id in `twitter_archive`. With the use of a loop and handling exceptions, tweets whcih were successful were saved in tweet_json.txt file. After which the last mentioned file had to be loaded as json to extract the values of specified keys in to a dataframe. Next was the use of programmatic assesment on the data using functions such head(), tail(), info(), value_counts(), duplicated() on each datasets to have a filter of values and a more indepth knowledge also verfying some thougths from the visual assessment. Using the *Define-Code-Test* structure each of the issues found were documented into quality and tidiness issues to be fixed in the Code section. The code section posed a few challenges and the use of various functions I have not used before. The code section also followed the *Define-code-Test* structure with each documented issue being split into individual issues following the above structure. The code stage include the use of chainging datatype, melt() fuction to combine doggo, puppo, pupper, and floofer columns into one taking care of an already documented issue. The need to drop columns which were of no need to this project was also performed in this section with the use of drop(). Replacing values was also performed as seen in the case of 'a', 'the', and 'an' being changed to NaN due to the incorrect assignemnt of names in the `twitter_archive` dataset. Additonal functions and actions were also taken to clean the mentioned issues documented earlier in the project. After which the clean data was stored. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
